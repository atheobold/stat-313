---
title: "Predicting Professor Evaluation Scores"
author: "Your group's names here!"
format: html
editor: visual
---

```{r set-up}
library(tidyverse)
library(moderndive)

# This is needed to make sure everyone gets the SAME testing / training datasets!
set.seed(1234)

evals <- evals |> 
  mutate(large_class = if_else(cls_students > 100, 
                               "large class", 
                               "regular class"), 
         eval_completion = cls_did_eval / cls_students 
         ) |> 
  select(-cls_did_eval, 
         -cls_students)
```

## Your Challenge

This week you have learned about model selection. During class you worked on performing a backward selection process to determine the "best" model for penguin body mass. 

Today, you are going to use **forward selection** to determine the "best" model for professor's evaluation score. There is an extra layer to this challenge, you are going to test how well your model predicts the evaluation scores of professors **not** in your dataset. The group(s) with the lowest prediction error will win **two** extra tokens. 

This task will require you to fit **tons** of linear regressions. **You must be able to show me exactly how you got to your top model.** Meaning, I need to see a record of **every** model you fit and compared along the way. 


### Your Datasets

I've used the `slice_sample()` function to divide the `evals` dataset into two datasets:

1. `evals_train` contains 80% of the observations in the original `evals` dataset -- **this is the dataset you will use for your models**

2. `evals_test` contains the other 20% of observations -- **this is the dataset you will use for your predictions** 

```{r test-train}
evals_train <- slice_sample(evals, prop = 0.8)
evals_test <- anti_join(evals, evals_train, by = "ID")
```

## Forward Selection

The forward selection process starts with a model with **no** predictor variables. That means, this model predicts the *same* mean evaluation score for every professor. I've fit this model for you below!

```{r one-mean}
one_mean <- lm(score ~ 1, data = evals_train)
```

You can pull out the adjusted $R^2$ for this model using the `get_regression_summaries()` function. 

```{r one-mean-summary}
get_regression_summaries(one_mean)
```

Based on this output, we are starting with a **really** low adjusted $R^2$. So, things can only get better from here! 

### Step 2

**Rules: You can only add a variable to the model if it improves adjusted $R^2$ by at least 1% (0.01)**

Alright, so now we get cooking. The next step is to fit **every** model with **one** explanatory variable. I've provided a list of every explanatory variable you are allowed to consider!

- `prof_ID` -- identification variable for each professor
- `age` -- age of the professor
- `bty_avg` -- average beauty rating of the professor
- `gender` -- gender of the professor
- `ethnicity` -- ethnicity of the professor
- `language` -- language of school where professor received education
- `rank` -- rank of professor
- `pic_outfit` -- outfit of professor in picture
- `pic_color` -- color of professor's picture
- `large_class` -- whether the class had over 100 students
- `eval_completion` -- proportion of students who completed the evaluation
- `cls_level` -- class level

Woof, that's 11 different variables. That means, for this first round, you will need to compare the adjusted $R^2$ for 12 different models to decide what variable should be added. 

Every model you fit will have the *same* format:

```
name_of_model <- lm(score ~ <variable>, data = evals_train)
```

But, the name of the model will need to change. I've started the proces for you, using the naming style of `one_` followed by the variable name (e.g., `one_id`, `one_bty`, etc.).  

```{r all-one-variable-models}

one_id <- lm(score ~ prof_ID, data = evals_train)
one_age <- lm(score ~ age, data = evals_train)
one_bty <- lm(score ~ bty_avg, data = evals_train)
one_gender <- lm(score ~ gender, data = evals_train)
one_ethnicity <- lm(score ~ ethnicity, data = evals_train)
one_language <- lm(score ~ language, data = evals_train)

## Now, you need to fit the other six models! 


```

Alright, now that you've fit the models, you need to inspect the adjusted $R^2$ values to see which of these 11 models is the "top" model -- the model with the highest adjusted $R^2$! Similar to before, I've provided you with some code to get you started, but you need to write the remaining code. 

```{r one-variable_model-summaries}
get_regression_summaries(one_id)
get_regression_summaries(one_age)
get_regression_summaries(one_bty)
get_regression_summaries(one_gender)
get_regression_summaries(one_ethnicity)
get_regression_summaries(one_language)

## Now, you need to compare the other six models! 


```

**1. What model was your top model?**



### Step 3

Alright, you've added one variable, the next step is to decide if you should add a second variable. This process looks nearly identical to the previous step, with one major change:
**every model you fit needs to contain the variable you decided to add**. So, if you decided to add the `bty_avg` variable, every model you fit would look like this:

```
name_of_model <- lm(score ~ bty_avg + <variable>, data = evals_train)
```

Again, the name of the model will need to change. This round, you are on your own -- I've provided you with no code. Here are my recommendations:

- name each model `two_` followed by the names of both variables (e.g., `two_bty_id`)
- go through each variable step-by-step just like you did before

```{r adding-a-second-variable}
## Code fo fit all 12 models that add a second variable to your top model goes here!



```

Alright, now you should have 11 more models to compare! Like before, you need to inspect the adjusted $R^2$ values to see which of these 12 models is the "top" model. 

**Rules: You can only add a variable to the model if it improves adjusted $R^2$ by at least 1% (0.01) from the model you chose in Question 1.**

```{r two-variable-model-summaries}
## Code to compare all 12 models you fit goes here!



```


**2. What model was your top model?**


### Step 4

As you might have expected, in this step we add a *third* variable to our top model from the previous step. This process should be getting familiar at this point! 

This process looks nearly identical to the previous step, with one major change:
**every model you fit needs to contain BOTH variables you decided to add**. So, if you decided to add the `bty_avg` and `gender` variables, every model you fit would look like this:

```
name_of_model <- lm(score ~ bty_avg + gender + <variable>, data = evals_train)
```

Again, the name of the model will need to change. This round, you are on your own -- I've provided you with no code. Here are my recommendations:

- name each model `three_` followed by the names of the variables in the model (e.g., `three_bty_gender_`)
- go through each variable step-by-step just like you did before

```{r adding-a-third-variable}
## Code fo fit all 12 models that add a second variable to your top model goes here!



```

Alright, now you should have 12 more models to compare! Like before, you need to inspect the adjusted $R^2$ values to see which of these 12 models is the "top" model. 

**Rules: You can only add a variable to the model if it improves adjusted $R^2$ by at least 1% (0.01) from the model you chose in Question 2.**

```{r two-variable-model-summaries}
## Code to compare all 12 models you fit goes here!



```


**3. What model was your top model?**



## Testing Your Top Model

```{r}

```

