---
title: "Variable Selection in Multiple Regression"
format: 
  revealjs:
    self-contained: true
    theme: night
editor: visual
---


```{r set-up}
library(tidyverse)
library(moderndive)
```

## 

::: {style="font-size: 2.5em; color: #B6CADA;"}
**What is model selection?**
:::

. . .

</br>
</br>

::: {style="font-size: 2.5em; color: #B6CADA;"}
**Why use model selection?**
:::

## 

::: {style="font-size: 1.5em;"}
1. Lots of available predictor variables
:::

```{r}
#| echo: false

slice_sample(evals, n = 3) |> 
  knitr::kable() |> 
  kableExtra::kable_styling()
```


##

::: {style="font-size: 1.5em;"}
2. Interested in prediction not explanation
:::

. . .

> You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables $x$. You don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about $y$ using the information in $x$. 
>
> *ModernDive*

## 

::: {style="font-size: 2em; color: #B6CADA;"}
How do you use model selection?
:::

. . .

::: columns
::: {.column width="45%"}
- Stepwise Selection
  + Forward Selection
  + Backward Selection
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
- Resampling Methods
  + Cross Validation
  + Testing / Training Datasets
:::
:::

. . .

::: {style="font-size: 0.9em; color: #0F4C81;"}

**With any of these methods, you get to choose _how_ you decide if one model is better than another model.**
:::

## 

::: {style="font-size: 2em; color: #0F4C81;"}
$R^2$ -- Coefficient of Determination
:::

<!-- Wright was one of the giants of 20th century evolutionary biology. -->

::: columns
::: {.column width="45%"}
![](images/wright.jpg)

::: {style="font-size: 0.75em;"}
Wright, Sewall. 1921. Correlation and causation. Journal of Agricultural Research 20: 557-585.
:::
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

::: {style="font-size: 0.75em;"}
> 
>
> *Wikipedia*

:::
:::
:::

## 

::: {style="font-size: 2em; color: #0F4C81;"}
Adjusted $R^2$
:::

::: columns
::: {.column width="45%"}
![](images/ezekiel.jfif)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

::: {style="font-size: 0.75em;"}
> 
>
> *Wikipedia*

:::
:::
:::


## 

::: {style="font-size: 2em; color: #0F4C81;"}
p-values
:::

::: columns
::: {.column width="45%"}
![](images/fisher.jfif)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

::: {style="font-size: 0.75em;"}
> In null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. 
>
> *Wikipedia*

:::
:::
:::

## 

::: {style="font-size: 2em; color: #0F4C81;"}
AIC -- An Information Criterion
:::

::: columns
::: {.column width="45%"}
![](images/akaike.jpg)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}

::: {style="font-size: 0.75em;"}
> The Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models.
>
> *Wikipedia*

:::
:::
:::

##

```{r}
#| eval: false
full_penguin <- lm(body_mass_g ~ ., data = penguins)

small_penguin <- lm(body_mass_g ~ 1, data = penguins)

aic_model <- MASS::stepAIC(full_penguin, 
              scope = list(upper = full_penguin, 
                           lower = small_penguin), 
              direction = "forward", 
              trace = TRUE, 
              k = 2
              )
```

. . .

If you’ve ever assessed whether $\Delta$ AIC $> 2$ you have done something that is mathematically close to $p > 0.05$.

## 

::: {style="font-size: 2em; color: #B6CADA;"}
What should you use instead?
:::

> In fact, many statisticians discourage the use of stepwise regression alone for model selection and advocate, instead, for a more thoughtful approach that carefully considers the research focus and features of the data.
>
> *Introduction to Modern Statistics*
