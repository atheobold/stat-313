---
title: "Lab 2: R Skills Refresher"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(okcupiddata)
library(infer)

data("profiles")

```

## Data 

The data consists of the public profiles of 59,946 OkCupid users who were living within 25
miles of San Francisco, had active profiles on June 26, 2012, were online in the previous
year, and had at least one picture in their profile. 

Variables include typical user information (such as sex, sexual orientation, age, and ethnicity) and lifestyle variables (such as diet, drinking habits, smoking habits). 


### Inspecting Data for Missing Values

The `drop_na()` function is a handy tool to remove any rows with missing values from a dataset! 

For these data, all missing values have been coded as `NA`, so R knows what to do with them. Let's see how many rows in the dataset have missing values. To do this we need two pieces of information:

1. How many rows are in the original dataset
2. How many rows are in the dataset with the `NA`s removed

I'll start the first task, and you figure out the second. 

```{r missing-values, exercise = TRUE}
## Number of rows and columns from glimpse()
glimpse(profiles)

## Number of rows and columns from dim()
dim(profiles)

## Number of rows and columns for data with NAs removed


```


Now, let's save the dataset with the missing values removed as a new dataset named `profiles_clean`. 

```{r clean, exercise = TRUE}

profiles_clean <- _______

```


## Visualizing Relationships

Let's suppose we're interested in the relationship between `height` and `age`. We can create a preliminary visualization of this relationship to assess if a linear regression would be a good choice.

```{r first-plot-setup, include = FALSE}
profiles_clean <- drop_na(profiles)
```

```{r first-plot, exercise = TRUE}
profiles_clean %>% 
  ggplot(aes(x = ___, y = ___)) + 
  ____
  
```


###

Boy! That clump of points is really dense! Let's use some tools to try and make it a bit better. 

```{r overplot-setup, include = FALSE}
profiles_clean <- drop_na(profiles)
```


```{r overplot, exercise = TRUE}
profiles_clean %>% 
  ggplot(aes(x = age, y = height)) + 
  geom_XXX(___)

```


## Hypothesis Testing

We've learned about the idea of shuffling the data to see what could have possibly happened under the null hypothesis. In a linear regression, this amounts to taking the (x, y) pairs and separating the x from the y. Then the x's get shuffled around and paired with new y-values. This is assumes there is no linear relationship between x and y (the null hypothesis). 

What this process looks like in R is shown in the code below: 

```{r, eval = FALSE}
data %>% 
  specify(y ~ x) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "slope")
```

Above, we see four steps, let's break them down:

Step 1: `specify()` the response and explanatory variables (note that these are being piped in from the `data`set)

Step 2: `hypothesize()` what would happen under the null (note this means that the *variables* are independent **not** the observations)

Step 3: `generate()` samples that would have happened under the null hypothesis, by `"permuting"` the x-values to new y-values

Step 4: `calculate()` the statistic you are interested in for each of the `generated()` samples


###

Your turn! Let's create a simulated null distribution to test the hypothesis that there is no relationship between age and height. 

```{r null-dist-setup, include = FALSE}
profiles_clean <- drop_na(profiles)
```

```{r null-dist, exercise = TRUE}
null_dist <- profiles_clean %>% 
  specify(height ~ age) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 5000, type = "permute") %>% 
  calculate(stat = "slope")

```

### 

The next step to obtaining our p-value is to find what the observed slope is. We can do this using the same tools as we used to make the null distribution, but we don't need to `hypothesize()` or `generate()`. 

```{r obs-slope-setup, include = FALSE}
profiles_clean <- drop_na(profiles)
```

```{r obs-slope, exercise = TRUE}
obs_slope <- profiles_clean %>% 
  specify(height ~ age) %>% 
  calculate(stat = "slope")

```

### 

Finally, we can find the p-value! We need to find where the observed statistic falls on the null distribution, and count how many statistics (slopes) are as big or bigger than the observed slope. We don't need to count this, R can do it for us! 

```{r p-value, exercise = TRUE}

get_p_value(null_dist, obs_stat = obs_slope, direction = "both")

null_dist %>% 
  visualise()
```


## Confidence Interval 

Alternatively, if we wanted to know what range of values the population parameter might take on, a confidence interval would be a better choice. 

To estimate a plausible range of values, we are going to use resampling to see what other samples from the population might look like. 

When we do this, our steps look like the following:

```{r, eval = FALSE}
data %>% 
  specify(y ~ x) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope")
```


Notice, that the `hypothesize()` step is gone. That is because we are not assuming the null hypothesis is true when we are estimating the confidence interval. We want to know what other samples could have looked like (not samples under the null hypothesis). We also need to change the samples we are generating from `"permute"` to `"bootstrap"`. Specifying `"bootstrap"` tells R that we are interested in **resampling** from the sample, not **reshuffling** the sample into new (x, y) pairs.   

Your turn! Let's create a simulated bootstrap distribution to estimate the true relationship between age and height. 

```{r boot-dist-setup, include = FALSE}
profiles_clean <- drop_na(profiles)
```

```{r boot-dist, exercise = TRUE}
boot_dist <- profiles_clean %>% 
  specify(height ~ age) %>% 
  generate(reps = 2000, type = "bootstrap") %>% 
  calculate(stat = "slope")

boot_dist %>% 
  visualise()
```

### 

The next step to obtaining our confidence interval is to determine what percentage of statistics we want to keep in the confidence interval. 90%? 95%? 99%? 80%? 

```{r confidence-interval, exercise = TRUE}
get_confidence_interval(boot_dist, level = 0.95, type = "percentile")
```


### 

Would you have reached the same conclusion for your hypothesis test with the confidence interval you created? 





