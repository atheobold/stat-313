---
title: "Inference for Regression"
date: "November 4, 2020"
output: ioslides_presentation
fig_height: 4
fig_width: 4
---

<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r setup, include=FALSE}
set.seed(12345)
knitr::opts_chunk$set(warning = FALSE, 
                       message = FALSE, 
                      fig.width = 5, 
                      fig.height = 4, 
                      results = "hold")

options(show.signif.stars = FALSE)

library(knitr)
library(tidyverse)
library(moderndive)
library(openintro)
library(infer)
library(flair)
library(broom)
library(gridExtra)
library(kableExtra)

evals_small <- evals %>% 
  group_by(prof_id) %>% 
  sample_n(size = 1) %>% 
  ungroup()

obs_slope <- evals_small %>% 
  specify(score ~ age) %>% 
  calculate(stat = "slope")

bootstrap_slope <- evals_small %>% 
  specify(formula = score ~ age) %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope")
```

<!-- <div class="columns-2"> -->

## Recall: Evals Data 

These data contain observations on 463 courses at the University of Texas 
Austin, with 23 variables recorded in the data:

- information about the course
- information about the professor
- information about number of students 
- information about the "attractiveness" rating of the professor

Data from the 94 unique professors was randomly sampled, so that each row in the 
dataset corresponds to one course taught by that professor. 

## Recall: Linear Regression 

We can model the relationship between a professor's evaluation score (`score`)
and their average attractiveness score (`age`) with a simple linear 
regression!

- Scores for average beauty can range from 0 to 10
- Ages range from 29 to 73, with a median of 47

## Visualization

```{r slr-viz, echo = FALSE, fig.width=8}
slr <- evals %>% 
  ggplot(aes(x = age, y = score)) + 
  geom_jitter() + 
  geom_smooth(method = "lm")


mlr <- evals %>% 
  ggplot(aes(x = age, y = score, color = gender)) + 
  geom_jitter() + 
  geom_smooth(method = "lm")

grid.arrange(slr, mlr, widths = c(0.45, 0.55))

```

## Fitting a Model 

```{r}
slr <- lm(score ~ age, data = evals) 
get_regression_table(slr)
```

## Interpreting the Model 

Using the values in the estimate column of the regression table we can obtain
the equation of the “best-fitting” regression line from before: 

$$
\widehat{\text{eval}} = 4.46 -0.006 \cdot \text{age}
$$
</br> 
<center> 

__How would we interpret the slope on `age`?__ 

__What parameter is this an estimate of?__ 

## SE and Sampling Variability 

<div class="columns-2">

```{r, echo = FALSE}
get_regression_table(slr) %>% 
  select(estimate, std_error)
```
- The standard error is the standard deviation of a point estimate calculated
from the sample. 

</div>
</br>

- A slope of -0.006 is just one possible value of the fitted slope. 
  * If we collected a different sample of 94 professors we would almost
  certainly obtain a different fitted slope. 

- The variability of the sampling distribution is also called the standard error! 
  * The standard error quantifies how much variation in the fitted slope one
  would expect between different samples.

## Confidence Intervals  

```{r, echo = FALSE}
get_regression_table(slr) %>% 
  select(lower_ci, upper_ci)
```

</br> 

- The two rightmost columns of the regression table correspond to the endpoints
of the 95% confidence interval for the population slope $\beta_1$. 
  * A confidence interval can be thought of as a "net" for the population
  parameter. 
  * A confidence interval of (-0.011, -0.001) is a range of plausible values for
  $\beta_1$.

## Calculation of Confidence Interval 

- `R` uses theory-based methods to acquire confidence intervals. 

- Assuming that the sampling distribution is bellshaped, we can calculate a 95% 
confidence interval as: 

$$ 
b_1 \pm 1.96 \cdot \text{SE}_{b_1} = -0.006 \pm 1.96 \cdot 0.003 \\ = (-0.0118, -0.0001)
$$
```{r, echo = FALSE}
get_regression_table(slr) %>% 
  select(lower_ci, upper_ci)
```


## Connection with Bootstrapping 

- Bootstrapping re-samples from the original sample to see what other possible 
samples could have looked like. 
  * The slope for each of these re-samples gives us a better idea of the 
  sampling variability of the slope. 
  
</br> 

__What is the key assumption about the original sample when bootstrapping?__ 

</br>

- Bootstrapping with replacement is done row-by-row, so the original pairs of
`score` and `age` are always kept together.

## Finding Bootstrap Statistics 

```{r boot, eval=FALSE, echo=FALSE}
bootstrap_slope <- evals_small %>% 
  specify(formula = score ~ age) %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope")

bootstrap_slope
```

```{r, echo = FALSE}
decorate_chunk("boot") %>% 
  flair_funs()
```

## Calculating a Confidence Interval

```{r}
percentile_ci <- get_confidence_interval(bootstrap_slope, level = 0.95)
se_ci <- get_confidence_interval(bootstrap_slope, level = 0.95, 
                                 type = "se", point_estimate = obs_slope)
```

```{r, echo = FALSE}
visualize(bootstrap_slope) + 
  labs(x = "Re-sampled Slope", 
       title = "Bootstrap Distribution") +
  shade_confidence_interval(endpoints = percentile_ci, fill = NULL, 
                            linetype = "solid", color = "dark blue") + 
  shade_confidence_interval(endpoints = se_ci, fill = NULL, 
                            linetype = "dashed", color = "dark green") +
  shade_confidence_interval(endpoints = c(-0.011, -0.001), fill = NULL, 
                            linetype = "dotted", color = "black")
```

## Confidence Intervals & Hypothesis Tests 

```{r, echo = FALSE}
t_ci <- get_regression_table(slr) %>%
    filter(term == "age") %>% 
  select(lower_ci, upper_ci)

confidence_intervals <- rbind(percentile_ci, 
                              se_ci, 
                              t_ci) %>% 
  mutate(type = c("Percentile", "SE", "Theoretical")) %>% 
  select(type, lower_ci, upper_ci)

confidence_intervals

```

- A confidence provides a range of plausible values for the population parameter, 
here $\beta_1.$ 
  * If the null value ($\beta_1 = 0$) is contained in the interval, then 0 is 
a plausible value for $\beta_1.$ 

<center> 

__What would you conclude for your hypothesis test?__ 


## Hypothesis Testing for the Slope

The statistic on the regression table tests between the hypotheses: 

$$ H_0: \beta_1 = 0 \\
H_A: \beta_1 \neq 0
$$

_OR_

<center> 

$H_0:$  there is no true (linear) relationship between evaluation scores and age for all the instructors in our population  

$H_A:$ there is a (linear) relationship between evaluation scores and age for all the instructors in our population

## Test Statistic

```{r, echo = FALSE}
get_regression_table(slr) %>% 
  filter(term == "age") %>% 
  select(term, statistic, p_value)
```

- The statistic on the regression table is a $t$-statistic, calculated according
to the formula in the book. 


<!-- $$ -->
<!-- t = \frac{b_1 - \beta_1}{SE_{b_1}} \\ = \frac{b_1 - 0}{\frac{\frac{s_y}{s_x}{\sqrt{1 - r^2}}}{\sqrt{n - 2}}} -->
<!-- $$ -->

- To compute the p-value, `R` compares the test statistic of -2.31 to the
appropriate null distribution.
  * It can be mathematically proven that the null distribution of this statistic 
  is a  $t$-distribution
  * The distribution has degrees of freedom equal to $n - 2 = 94 - 2$.  

## Let's Try Simulation Instead 

```{r null, eval=FALSE, echo=FALSE}
null_slope <- evals_small %>% 
  specify(formula = score ~ age) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "slope")

null_slope
```

```{r, echo = FALSE}
decorate_chunk("null") %>% 
  flair_funs()
```

## Calculating a p-value 

<div class="columns-2">

```{r, echo = FALSE, fig.width=4}
null_slope %>% 
  visualise() + 
  shade_p_value(obs_stat = obs_slope, direction = "two-sided") + 
  labs(x = "Permuted Slope", 
       title = "Null Distribution")
```

```{r, echo = FALSE}
get_p_value(null_slope, obs_stat = obs_slope, direction = "two-sided")
```
</br> 

- What would you conclude for your hypothesis test? 
- Using a $t$-distribution gets a p-value of 0.021. Why the difference? 

</div>
</br>

- Would you have reached the same conclusion with your confidence interval? 

<!-- ## Why Bootstrap and Permute?  -->

<!-- - Theory-based methods and simulation-based methods for constructing confidence -->
<!-- intervals and conducting hypothesis tests often yield consistent results.  -->
<!-- - Simulation based methods: -->
<!--   * are more robust to violations of the normality condition -->
<!--   * are easier for people to understand -->
<!--   * they also work in situations where theory-based methods and mathematical  -->
<!--   formulas don’t exist. -->




