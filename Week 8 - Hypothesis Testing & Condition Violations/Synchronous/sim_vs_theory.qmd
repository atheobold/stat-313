---
title: "`r emo::ji('scientist')` Simulation-Based Methods versus Theory-Based Methods"
format: 
  revealjs:
    theme: dark
    self-contained: true
editor: visual
---

```{r set-up}
library(tidyverse)
library(infer)
library(moderndive)
library(lterdatasampler)

my_theme <- theme(axis.title.x = element_text(size = 18), 
                  axis.title.y = element_text(size = 18), 
                  axis.text.x = element_text(size = 12), 
                  axis.text.y = element_text(size = 12))

obs_slope <- evals %>% 
  specify(response = score, 
          explanatory = bty_avg) %>% 
  calculate(stat = "slope")
```

::: {style="font-size: 4em; color: #FFFFFF;"}
What did we just do?
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
We carried out a hypothesis test!
:::

::: columns
::: {.column width="35%"}
$$H_0: \beta_1 = 0$$

$$H_A: \beta_1 \neq 0$$
:::

::: {.column width="5%"}
:::

::: {.column width="60%"}
```{r evals-slr}
#| message: false
#| fig-align: center

ggplot(data = evals, 
       mapping = aes(x = bty_avg, y = score)) +
  geom_jitter() + 
  geom_smooth(method = "lm") +
  labs(x = "Average Beauty Score", 
       y = "Course Evaluation Score") +
  my_theme
```
:::
:::

::: {style="font-size: 1em; color: #B6CADA;"}
What do these hypotheses mean *in words*?
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
By creating a permutation distribution!
:::

```{r evals-permute}
#| echo: true
null_dist <- evals %>% 
  specify(response = score, 
          explanatory = bty_avg) %>% 
  hypothesise(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "slope")
```

. . .

</br>

::: {style="font-size: 1.5em; color: #B6CADA;"}
What is happening in the `generate()` step?
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
And visualizing where our observed statistic fell on the distribution
:::

::: columns
::: {.column width="60%"}
```{r}
visualise(null_dist) +
  shade_p_value(obs_stat = obs_slope, direction = "two-sided") +
  labs(x = "Permuted Slope Statistic")
```
:::

::: {.column width="5%"}
:::

::: {.column width="35%"}
::: {style="font-size: 1.5em; color: #B6CADA;"}
What would you estimate the p-value to be?
:::
:::
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
And calculated the p-value
:::

::: columns
::: {.column width="60%"}
```{r}
#| echo: true

get_p_value(null_dist, 
            obs_stat = obs_slope, 
            direction = "two-sided")
```
:::

::: {.column width="5%"}
:::

::: {.column width="35%"}
::: {style="font-size: 1.5em; color: #B6CADA;"}
</br> </br> What would you decide for your hypothesis test?
:::
:::
:::

## 

::: {style="font-size: 2.5em; color: #FFFFFF;"}
How would this process have changed if we used theory-based methods instead?
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
Approximating the permutation distribution
:::

::: columns
::: {.column width="40%"}
A $t$-distribution can be a reasonable approximation for the permutation distribution if certain conditions are not violated.
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r t-distribution}
plot(c(-5, 5),
     c(0, dnorm(0)),
      type = "n", 
     ylab = "", 
     xlab = "",
     axes = FALSE
     )

at <- seq(-10, 10, 2)
axis(1, at)
axis(1, at - 1, rep("", length(at)), tcl = -0.1)
abline(h = 0)
COL. <- openintro::fadeColor(openintro::IMSCOL["blue", "full"], c("FF", "89", "68", "4C", "33"))
COLt <- openintro::fadeColor(openintro::IMSCOL["blue", "full"], c("FF", "AA", "85", "60", "45"))
DF <- c("normal", 8, 4, 2, 1)
X <- seq(-10, 10, 0.02)
Y <- dnorm(X)
lines(X, Y, col = COL.[1])
for (i in 2:5) {
  Y <- dt(X, as.numeric(DF[i]))
  lines(X, Y, col = COL.[i], lwd = 1.5)
}
legend(2.5, 0.4,
  legend = c(
    DF[1],
    paste("t, df = ", DF[2:5], sep = "")
  ),
  col = COL.,
  text.col = COLt,
  lty = rep(1, 5),
  lwd = 1.5
)

```

:::
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
What about the observed statistic?
:::

::: {.panel-tabset}

## Before 

```{r obs-slop}
#| echo: true
obs_slope <- evals %>% 
  specify(response = score, 
          explanatory = bty_avg) %>% 
  calculate(stat = "slope")
```

```{r}
obs_slope
```

## Now

```{r obs-t}
#| echo: true

evals_lm <- lm(score ~ bty_avg,
               data = evals)

get_regression_table(evals_lm)
```

:::

##

::: {style="font-size: 2em; color: #FFFFFF;"}
How did R calculate the $t$-statistic?
:::

::: {.panel-tabset}

## Step 1: SE

::: columns
::: {.column width="40%"}
$SE_{b_1} = \frac{\frac{s_y}{s_x} \cdot \sqrt{1 - r^2}}{\sqrt{n - 2}}$
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r}
se_num <- ( sd(evals$score) / sd(evals$bty_avg) ) * 
  sqrt(
    1 - cor(evals$score, evals$bty_avg)
    ) 

se_denom <- sqrt(nrow(evals) - 2)

se <- se_num / se_denom

se
```

:::
:::

## Step 2: t-statistic

::: columns
::: {.column width="40%"}
$t = \frac{b_1}{SE_{b_1}}$
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
```{r}
t <- pull(obs_slope) / se

t
```

:::
:::

## Proof!
```{r}
get_regression_table(evals_lm)
```
:::

##

::: {style="font-size: 2em; color: #FFFFFF;"}
How does R calculate the p-value?
:::

. . .

```{r}
#| fig-align: center
#| out-width: 50%

ggplot(data = tibble(x = c(-5, 5)), 
       mapping = aes(x)) +
  stat_function(fun = dt, 
                args = list(df = nrow(evals) - 2), 
                color = "blue", 
                linewidth = 1.5) +
  ylab("") +
  scale_y_continuous(breaks = NULL) +
  geom_vline(xintercept = t, color = "red", linewidth = 1.5)

```

. . .

**How many degrees of freedom does this $t$-distribution have?**

##

::: {style="font-size: 3.5em; color: #B6CADA;"}
Did we get similar results between these methods?
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
Why not always use theoretical methods?
:::

. . .

::: columns
::: {.column width="45%"}
Theory-based methods only hold if the sampling distribution is normally shaped.
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
The normality of a sampling distribution depends **heavily** on model conditions.
:::
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
What are these "conditions"?
:::

. . .

::: columns
::: {.column width="40%"}
For linear regression we are assuming...
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
**L**inear relationship between $x$ and $y$

</br>

**I**ndepdent observations

</br>

**N**ormality of residuals

</br>

**E**qual variance of residuals
:::
:::

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
**L**inear relationship between $x$ and $y$
:::

```{r}
#| fig-align: center

and_vertebrates %>%
  filter(species != "Cascade torrent salamander") %>% 
  ggplot(mapping = aes(x = length_1_mm, 
                       y = weight_g, 
                       color = species)) +
  geom_point() + 
  labs(x = "Length (mm)", 
       y = "Weight (g)") +
  my_theme
```

. . .

**What should we do?**

##

::: {style="font-size: 2em; color: #FFFFFF;"}
Variable transformation!
:::

```{r transform}
#| fig-align: center

and_vertebrates %>%
  filter(species != "Cascade torrent salamander") %>% 
  ggplot(mapping = aes(x = length_1_mm, 
                       y = weight_g, 
                       color = species)) +
  geom_point() + 
  labs(x = "Log Transformed Length (mm)", 
       y = "Log Transformed Weight (g)") +
  scale_x_log10() +
  scale_y_log10() +
  my_theme
```


##

::: {style="font-size: 2em; color: #FFFFFF;"}
**I**ndependence of observations
:::

::: columns
::: {.column width="40%"}
The `evals` dataset contains `r nrow(evals)` observations on `r distinct(evals, prof_ID) %>% nrow()` professors. Meaning, professors have **multiple** observations. 

</br>

*What can we do?*
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
**Best** -- use a random effects model

**Reasonable** -- collapse the multiple scores into a single score
:::
:::

##

::: {style="font-size: 2em; color: #FFFFFF;"}
**N**ormality of residuals
:::

```{r non-normal}

fish <- and_vertebrates %>%
  filter(!species %in% c("Cascade torrent salamander", "Coastal giant salamander")) 

curved_lm <- lm(weight_g ~ length_1_mm, data = fish)

get_regression_points(curved_lm) %>% 
  ggplot(mapping = aes(x = residual)) + 
  geom_histogram() + 
  labs(x = "Residual") +
  my_theme
```

. . .

**What should we do?**

##

::: {style="font-size: 2em; color: #FFFFFF;"}
Variable transformation!
:::

```{r normal}

fish <- and_vertebrates %>%
  filter(!species %in% c("Cascade torrent salamander", "Coastal giant salamander"))

not_curved_lm <- lm(log(weight_g) ~ log(length_1_mm), data = fish)

broom::augment(not_curved_lm) %>% 
  ggplot(mapping = aes(x = .resid)) + 
  geom_histogram(binwidth = 0.1) + 
  labs(x = "Residual") +
  xlim(c(-0.5, 0.5)) +
  my_theme
```


##

::: {style="font-size: 2em; color: #FFFFFF;"}
**E**qual variance of residuals
:::

```{r non-constant-variance}
#| fig-align: center

ggplot(data = hbr_maples, 
       mapping = aes(x = stem_length, y = stem_dry_mass)
       ) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    x = "Stem Length (mm)",
    y = "Stem Dry Mass (g)",
    title = "Sugar Maple Seedlings in Hubbard Brook LTER"
  ) +
  theme_minimal() + 
  my_theme
```

. . .

**What should we do?**

## 

::: {style="font-size: 2em; color: #FFFFFF;"}
Variable transformation!
:::

```{r}
ggplot(data = hbr_maples, 
       mapping = aes(x = stem_length, y = stem_dry_mass)
       ) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(
    x = "Stem Length (mm)",
    y = "Log Transformed Stem Dry Mass (g)",
    title = "Sugar Maple Seedlings in Hubbard Brook LTER"
  ) +
  theme_minimal() + 
  scale_y_log10() +
  my_theme
```


##

::: {style="font-size: 2em; color: #FFFFFF;"}
Are these conditions required for **both** methods?
:::

. . .

::: columns
::: {.column width="40%"}
::: {style="font-size: 1.5em; color: #B6CADA;"}
Simulation-based Methods
:::

::: {style="font-size: 0.75em; color: #FFFFFF;"}
- Linearity of Relationship

- Independence of Observations

- Equal Variance of Residuals
:::
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
::: {style="font-size: 1.5em; color: #B6CADA;"}
Theory-based Methods
:::

::: {style="font-size: 0.75em; color: #FFFFFF;"}
- Linearity of Relationship
- Independence of Observations
:::

::: {style="font-size: 0.75em; color: #B6CADA;"}
- Normality of Residuals
:::

::: {style="font-size: 0.75em; color: #FFFFFF;"}
- Equal Variance of Residuals
:::

:::
:::

##

::: {style="font-size: 2em; color: #FFFFFF;"}
What happens if the conditions are violated?
:::

. . .

In general, when the conditions associated with these methods are violated, the permutation and $t$-distributions will underestimate the true standard error of the sampling distribution.
